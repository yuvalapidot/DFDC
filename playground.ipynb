{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import math\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.cm as cm\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "plt.rcParams[\"image.cmap\"] = 'hsv'\n",
    "plt.rcParams[\"animation.embed_limit\"] = 1866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MTCNN in module facenet_pytorch.models.mtcnn:\n",
      "\n",
      "class MTCNN(torch.nn.modules.module.Module)\n",
      " |  MTCNN(image_size=160, margin=0, min_face_size=20, thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True, select_largest=True, keep_all=False, device=None)\n",
      " |  \n",
      " |  MTCNN face detection module.\n",
      " |  \n",
      " |  This class loads pretrained P-, R-, and O-nets and, given raw input images as PIL images,\n",
      " |  returns images cropped to include the face only. Cropped faces can optionally be saved to file\n",
      " |  also.\n",
      " |  \n",
      " |  Keyword Arguments:\n",
      " |      image_size {int} -- Output image size in pixels. The image will be square. (default: {160})\n",
      " |      margin {int} -- Margin to add to bounding box, in terms of pixels in the final image. \n",
      " |          Note that the application of the margin differs slightly from the davidsandberg/facenet\n",
      " |          repo, which applies the margin to the original image before resizing, making the margin\n",
      " |          dependent on the original image size (this is a bug in davidsandberg/facenet).\n",
      " |          (default: {0})\n",
      " |      min_face_size {int} -- Minimum face size to search for. (default: {20})\n",
      " |      thresholds {list} -- MTCNN face detection thresholds (default: {[0.6, 0.7, 0.7]})\n",
      " |      factor {float} -- Factor used to create a scaling pyramid of face sizes. (default: {0.709})\n",
      " |      post_process {bool} -- Whether or not to post process images tensors before returning. (default: {True})\n",
      " |      select_largest {bool} -- If True, if multiple faces are detected, the largest is returned.\n",
      " |          If False, the face with the highest detection probability is returned. (default: {True})\n",
      " |      keep_all {bool} -- If True, all detected faces are returned, in the order dictated by the\n",
      " |          select_largest parameter. If a save_path is specified, the first face is saved to that\n",
      " |          path and the remaining faces are saved to <save_path>1, <save_path>2 etc.\n",
      " |      device {torch.device} -- The device on which to run neural net passes. Image tensors and\n",
      " |          models are copied to this device before running forward passes. (default: {None})\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MTCNN\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, image_size=160, margin=0, min_face_size=20, thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True, select_largest=True, keep_all=False, device=None)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  detect(self, img, landmarks=False)\n",
      " |      Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\n",
      " |      \n",
      " |      This method is used by the forward method and is also useful for face detection tasks\n",
      " |      that require lower-level handling of bounding boxes and facial landmarks (e.g., face\n",
      " |      tracking). The functionality of the forward function can be emulated by using this method\n",
      " |      followed by the extract_face() function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          img {PIL.Image or list} -- A PIL image or a list of PIL images.\n",
      " |      \n",
      " |      Keyword Arguments:\n",
      " |          landmarks {bool} -- Whether to return facial landmarks in addition to bounding boxes.\n",
      " |              (default: {False})\n",
      " |      \n",
      " |      Returns:\n",
      " |          tuple(numpy.ndarray, list) -- For N detected faces, a tuple containing an\n",
      " |              Nx4 array of bounding boxes and a length N list of detection probabilities.\n",
      " |              Returned boxes will be sorted in descending order by detection probability if\n",
      " |              self.select_largest=False, otherwise the largest face will be returned first.\n",
      " |              If `img` is a list of images, the items returned have an extra dimension\n",
      " |              (batch) as the first dimension. Optionally, a third item, the facial landmarks,\n",
      " |              are returned if `landmarks=True`.\n",
      " |      \n",
      " |      Example:\n",
      " |      >>> from PIL import Image, ImageDraw\n",
      " |      >>> from facenet_pytorch import MTCNN, extract_face\n",
      " |      >>> mtcnn = MTCNN(keep_all=True)\n",
      " |      >>> boxes, probs, points = mtcnn.detect(img, landmarks=True)\n",
      " |      >>> # Draw boxes and save faces\n",
      " |      >>> img_draw = img.copy()\n",
      " |      >>> draw = ImageDraw.Draw(img_draw)\n",
      " |      >>> for i, (box, point) in enumerate(zip(boxes, points)):\n",
      " |      ...     draw.rectangle(box.tolist(), width=5)\n",
      " |      ...     for p in point:\n",
      " |      ...         draw.rectangle((p - 10).tolist() + (p + 10).tolist(), width=10)\n",
      " |      ...     extract_face(img, box, save_path='detected_face_{}.png'.format(i))\n",
      " |      >>> img_draw.save('annotated_faces.png')\n",
      " |  \n",
      " |  forward(self, img, save_path=None, return_prob=False)\n",
      " |      Run MTCNN face detection on a PIL image. This method performs both detection and\n",
      " |      extraction of faces, returning tensors representing detected faces rather than the bounding\n",
      " |      boxes. To access bounding boxes, see the MTCNN.detect() method below.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          img {PIL.Image or list} -- A PIL image or a list of PIL images.\n",
      " |      \n",
      " |      Keyword Arguments:\n",
      " |          save_path {str} -- An optional save path for the cropped image. Note that when\n",
      " |              self.post_process=True, although the returned tensor is post processed, the saved face\n",
      " |              image is not, so it is a true representation of the face in the input image.\n",
      " |              If `img` is a list of images, `save_path` should be a list of equal length.\n",
      " |              (default: {None})\n",
      " |          return_prob {bool} -- Whether or not to return the detection probability.\n",
      " |              (default: {False})\n",
      " |      \n",
      " |      Returns:\n",
      " |          Union[torch.Tensor, tuple(torch.tensor, float)] -- If detected, cropped image of a face\n",
      " |              with dimensions 3 x image_size x image_size. Optionally, the probability that a\n",
      " |              face was detected. If self.keep_all is True, n detected faces are returned in an\n",
      " |              n x 3 x image_size x image_size tensor with an optional list of detection\n",
      " |              probabilities. If `img` is a list of images, the item(s) returned have an extra \n",
      " |              dimension (batch) as the first dimension.\n",
      " |      \n",
      " |      Example:\n",
      " |      >>> from facenet_pytorch import MTCNN\n",
      " |      >>> mtcnn = MTCNN()\n",
      " |      >>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.data.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  buffers(self, recurse=True)\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf.data), buf.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix='', recurse=True)\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse=True)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |      \n",
      " |      .. warning ::\n",
      " |      \n",
      " |          The current implementation will not have the presented behavior\n",
      " |          for complex :class:`Module` that perform many operations.\n",
      " |          In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only\n",
      " |          contain the gradients for a subset of the inputs and outputs.\n",
      " |          For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`\n",
      " |          directly on a specific input or output to get the required gradients.\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self, requires_grad=True)\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point desired :attr:`dtype` s. In addition, this method will\n",
      " |      only cast the floating point parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MTCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def video_read(video_path):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    buf = np.empty((frame_count, frame_height, frame_width, 3), np.dtype('uint8'))\n",
    "    fc = 0\n",
    "    ret = True\n",
    "    while fc < frame_count and ret:\n",
    "        ret, frame = video.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            buf[fc] = frame\n",
    "            fc += 1\n",
    "    video.release()\n",
    "    return fc, buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def video_write(buf, out_path):\n",
    "    out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc(*'mp4v'), 30, (buf.shape[2], buf.shape[1]), 0)\n",
    "    for frame in buf:\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "    print('done - ' + out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def draw_videos(videos_cols, columns_color_maps, columns_titles, fc):\n",
    "    cols = len(videos_cols)\n",
    "    rows = max(map(lambda col: len(col), videos_cols))\n",
    "    fig, ax = plt.subplots(nrows=rows, ncols=cols, squeeze=False, figsize=(10 * cols, 10 * rows))\n",
    "    #TO CHECK\n",
    "    im = [[ax[j, i].imshow(video[0], cmap=columns_color_maps[i], animated=True)\n",
    "           for j, video in enumerate(col)] \n",
    "          for i, col in enumerate(videos_cols)]      \n",
    "    for i, title in enumerate(columns_titles): ax[0][i].title.set_text(title)\n",
    "    def draw(frame_count):\n",
    "        # TO CHECK\n",
    "        [im[i][j].set_data(video[frame_count])\n",
    "         for i, col in enumerate(videos_cols)\n",
    "         for j, video in enumerate(col)]\n",
    "    ani = animation.FuncAnimation(fig, draw, interval=30, save_count=fc)\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def detect_faces(video, device):\n",
    "    mtcnn = MTCNN(keep_all=True, select_largest=False, post_process=True, device=device)\n",
    "    return [mtcnn.detect(Image.fromarray(frame)) for frame in video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_face_info(segments):\n",
    "    total_probability_threshold = 0.5\n",
    "    faces = []\n",
    "    def insert_info(info, frame_num, rect, proba):\n",
    "        info[\"last_frame\"] = frame_num\n",
    "        info[\"widths\"][frame_num] = rect[2] - rect[0]\n",
    "        info[\"heights\"][frame_num] = rect[3] - rect[1]\n",
    "        info[\"centers_x\"][frame_num] = rect[0] + (info[\"widths\"][frame_num] / 2)\n",
    "        info[\"centers_y\"][frame_num] = rect[1] + (info[\"heights\"][frame_num] / 2)\n",
    "        info[\"probs\"][frame_num] = proba\n",
    "    for frame_num, frame_segment in enumerate(segments):\n",
    "        for rect, prob in zip(frame_segment[0], frame_segment[1]):\n",
    "            matched = False\n",
    "            for face_info in faces:\n",
    "                last_frame = face_info[\"last_frame\"]\n",
    "                if last_frame < frame_num:\n",
    "                    if rect[0] < face_info[\"centers_x\"][last_frame] < rect[2] \\\n",
    "                    and rect[1] < face_info[\"centers_y\"][last_frame] < rect[3]:\n",
    "                        insert_info(face_info, frame_num, rect, prob)\n",
    "                        matched = True\n",
    "                        break\n",
    "            if not matched:\n",
    "                face_info = {\n",
    "                    \"last_frame\": 0,\n",
    "                    \"centers_x\": np.zeros(len(segments), dtype='float32'),\n",
    "                    \"centers_y\": np.zeros(len(segments), dtype='float32'),\n",
    "                    \"widths\": np.zeros(len(segments), dtype='float32'),\n",
    "                    \"heights\": np.zeros(len(segments), dtype='float32'),\n",
    "                    \"probs\": np.zeros(len(segments), dtype='float32')\n",
    "                }\n",
    "                faces.append(face_info)\n",
    "                insert_info(face_info, frame_num, rect, prob)\n",
    "    faces = list(filter(lambda face: np.mean(face[\"probs\"]) > total_probability_threshold, faces))\n",
    "    def interp_array(arr):\n",
    "        l = len(arr)\n",
    "        fp = arr[arr > 0]\n",
    "        xp = (arr > 0) * (np.arange(l) + 1) \n",
    "        xp = xp[xp > 0] - 1\n",
    "        return np.interp(np.arange(l), xp, fp)\n",
    "    for face in faces:\n",
    "        face[\"centers_x\"] = interp_array(face[\"centers_x\"])\n",
    "        face[\"centers_y\"] = interp_array(face[\"centers_y\"])\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metadata(path):\n",
    "    metadata_file = open(path)\n",
    "    metadata = json.load(metadata_file)\n",
    "    metadata_file.close()\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_metadata(metadata, path):\n",
    "    with open(path, 'w') as output:\n",
    "        json.dump(metadata, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dir = \"D:\\\\Projects\\\\DFDC\\\\Data\\\\\"\n",
    "metadata_file_name = 'metadata.json'\n",
    "\n",
    "metadata = read_metadata(dir + metadata_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Video: hmpbhblarx.mp4\n",
      "Fake Videos: ['abzcmncrxu.mp4']\n",
      "Number of frames: 300\n"
     ]
    }
   ],
   "source": [
    "video_name = 'abzcmncrxu.mp4'\n",
    "video_info = metadata[video_name]\n",
    "video_sub_dir = video_info['dir']\n",
    "if video_info['label'] == 'FAKE':\n",
    "    fake_names = [video_name]\n",
    "    real_name = video_info['original']\n",
    "elif video_info['label'] == 'REAL':\n",
    "    real_name = video_name\n",
    "    fake_names = video_info['fakes']\n",
    "else:\n",
    "    real_name = video_name\n",
    "    fake_names = []\n",
    "print(\"Original Video: {0}\".format(real_name))\n",
    "print(\"Fake Videos: {0}\".format(fake_names))\n",
    "fake_order = 0\n",
    "\n",
    "real_fc, real_video = video_read(\"{}{}\\\\{}\".format(dir, video_sub_dir, real_name))\n",
    "fake_fc, fake_video = video_read(\"{}{}\\\\{}\".format(dir, video_sub_dir, fake_names[fake_order]))\n",
    "fc = min(real_fc, fake_fc)\n",
    "real_video = real_video[:fc]\n",
    "fake_video = fake_video[:fc]\n",
    "print(\"Number of frames: {0}\".format(fc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "segments = detect_faces(real_video, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 faces in video\n"
     ]
    }
   ],
   "source": [
    "faces = get_face_info(segments)\n",
    "print('Found {} faces in video'.format(len(faces)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "size = 300\n",
    "video_height = real_video.shape[1]\n",
    "video_width = real_video.shape[2]\n",
    "def top(y):\n",
    "    return 0 if y < size / 2 else video_height - size if y + (size / 2) > video_height else int(y - (size / 2))\n",
    "def bottom(y):\n",
    "    return size if y < size / 2 else video_height if y + (size / 2) > video_height else int(y + (size / 2))\n",
    "def left(x):\n",
    "    return 0 if x < size / 2 else video_width - size if x + (size / 2) > video_width else int(x - (size / 2))\n",
    "def right(x):\n",
    "    return size if x < size / 2 else video_width if x + (size / 2) > video_width else int(x + (size / 2))\n",
    "faces_crops = [[(frame, top(y), bottom(y), left(x), right(x))\n",
    "         for frame, (x, y) in enumerate(zip(face[\"centers_x\"], face[\"centers_y\"]))]\n",
    "         for face in faces]\n",
    "real_face_video = np.stack(\n",
    "    [np.stack(\n",
    "        [real_video[frame, top:bottom, left:right, :] for frame, top, bottom, left, right in crops]\n",
    "    ) for crops in faces_crops])\n",
    "fake_face_video = np.stack(\n",
    "    [np.stack(\n",
    "        [fake_video[frame, top:bottom, left:right, :] for frame, top, bottom, left, right in crops]\n",
    "    ) for crops in faces_crops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diff = np.sqrt(np.sum(np.square(np.subtract(real_face_video, fake_face_video)), axis=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff min: 0.0\n",
      "Diff max: 27.331300737432898\n"
     ]
    }
   ],
   "source": [
    "diff_min = np.min(diff)\n",
    "print(\"Diff min: {0}\".format(diff_min))\n",
    "diff_max = np.max(diff)\n",
    "print(\"Diff max: {0}\".format(diff_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 12.767145334803704\n"
     ]
    }
   ],
   "source": [
    "top_percentage = 5\n",
    "threshold = np.percentile(diff, 100 - (top_percentage / diff.shape[0]))\n",
    "print(\"Threshold: {0}\".format(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(diff.flatten(), bins=range(math.ceil(diff_max)))\n",
    "plt.plot([threshold, threshold], [0, 1000000], linestyle='--', scalex=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "gray_diff = (diff - diff_min) * (255 / (diff_max - diff_min))\n",
    "gray_threshold = (threshold - diff_min) * (255 / (diff_max - diff_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mask = gray_diff > gray_threshold\n",
    "filtered_diff = gray_diff * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "combined = [[real_face_video, filtered_diff, fake_face_video]]\n",
    "test = [video.shape\n",
    "for i, row in enumerate(combined) \n",
    "for j, video in enumerate(row)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "diff_ani = draw_videos([real_face_video, filtered_diff, fake_face_video], [cm.viridis, cm.gray, cm.viridis], [\"Original\", \"Diff\", \"Fake\"], fc)\n",
    "# diff_ani = draw_videos([real_face_video, filtered_diff], [cm.viridis, cm.gray], [\"Original\", \"Diff\"], fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "diff_ani"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DFDC",
   "language": "python",
   "name": "dfdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
